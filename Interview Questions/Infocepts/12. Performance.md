## 12. Why might a model with fewer columns still perform worse than one with more columns?

While the general advice is to remove unnecessary columns, performance is more nuanced than just column count. Here are the key reasons a "leaner" model might be slower:

*   **High Column Cardinality (The Main Culprit):**
    *   A model with 5 columns, including a `DateTime` (very high cardinality) and a unique `TransactionID` (extremely high cardinality), will likely be larger and slower than a model with 20 low-cardinality columns (e.g., boolean flags, category codes, year/month numbers).
    *   The memory footprint and processing overhead of high-cardinality columns are significant and often outweigh the simple count of columns.
*   **Complex DAX Measures:**
    *   A model can have few columns but contain dozens of complex DAX measures that use iterators (`SUMX`, `FILTER`), complex context transitions, or inefficient logic.
    *   These measures put a heavy load on the Formula Engine at query time, which can be a much bigger bottleneck than the model's size. A larger model with simpler `SUM`-based measures might perform better.
*   **Inefficient Data Model Structure:**
    *   A model with fewer tables and columns might be poorly designed, for instance, by using a single flat table (denormalized) instead of a proper star schema.
    *   While a star schema might have more columns overall (spread across dimension tables), its relationships are highly optimized for filtering performance in the VertiPaq engine.
    *   The use of bidirectional filters or weak/inactive relationships in a "simpler" model can also cripple performance.
*   **Data Types:**
    *   Using less optimal data types can also have an impact. For example, storing a year as a text string or a full `DateTime` value when a whole number would suffice increases the model size and can slow down calculations.
