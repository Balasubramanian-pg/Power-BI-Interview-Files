### 17. If you want to apply the same slicer across multiple pages, how will you achieve that?

You achieve this using the **Sync Slicers** feature.

**The Process:**

1. **Create and format the slicer** on one of the pages (your "source" page).
2. With the slicer visual selected, go to the **View** tab in the Power BI Desktop ribbon.
3. Check the box for the **Sync slicers** pane. This will open a new pane on the right.
4. The Sync slicers pane shows a list of all pages in your report. For each page, there are two checkboxes next to your selected slicer:
    - **Sync (Chain icon):** This controls the _filtering_. If you check this box for a page, any selection made on the slicer will filter the visuals on that page, even if the slicer isn't visible there.
    - **Visible (Eye icon):** This controls the _visibility_ of the slicer visual itself. If you check this box, a copy of the slicer will appear on that page.
5. **Configure as Needed:**
    - To have one master slicer on Page 1 that filters Pages 1, 2, and 3: Keep it visible only on Page 1, but check the "Sync" box for all three pages.
    - To have the exact same slicer appear and be in sync on Pages 1, 2, and 3: Check both the "Sync" and "Visible" boxes for all three pages.

This powerful feature ensures a consistent user experience and saves you from having to manually set filters on every page.



### 18. How will you handle data from different data sources like one is from Google Sheets, another is from SQL Server, and another is from a previous project dataset? What will be the approach to bring them into Power BI Desktop?

Power BI Desktop is designed to connect to multiple, disparate data sources and integrate them into a single model. The approach is to connect to each source individually using the "Get Data" feature.

**The Approach:**

1. **Open Power BI Desktop and go to the "Get Data" menu.**
2. **Connect to SQL Server:**
    - Choose `Get Data > SQL Server`.
    - Enter the Server name and, optionally, the Database name.
    - Choose the **Data Connectivity mode**:
        - **Import:** Pulls a copy of the data into Power BI. Faster performance, but data is static until refreshed.
        - **DirectQuery:** Leaves the data in the SQL Server. Power BI sends live queries. Slower, but data is real-time. (Choose Import unless the dataset is massive or real-time is mandatory).
    - Use the Navigator window to select the specific tables or views you need.
3. **Connect to Google Sheets:**
    - Choose `Get Data > Web`.
    - You need a public link to the Google Sheet. In Google Sheets, go to `File > Share > Publish to the web` and get the link, making sure to publish it as a CSV or Excel file.
    - Paste this URL into Power BI's "Web" connector.
    - _(Alternative/Robust Method)_: A more reliable method is to use a Power Automate flow to periodically save the Google Sheet as an Excel file into a SharePoint or OneDrive folder, and then use Power BI's native `SharePoint folder` or `Excel` connector. This avoids issues with public links.
4. **Connect to a Previous Project Dataset (Power BI Dataset):**
    - Choose `Get Data > Power BI datasets`.
    - A list of all datasets you have access to in the Power BI Service will appear.
    - Select the dataset from the previous project.
    - This will create a **Live Connection** to that published dataset. This means you are connecting to a pre-existing, curated data model. You cannot modify the data transformations or relationships in your new PBIX file; you can only build new report visuals on top of it. This promotes a "single source of truth."

**Integration:**  
Once all three sources are connected, they will appear as separate queries in the Power Query Editor. Here, you can perform any necessary cleaning and transformation. After clicking "Close & Apply," all the tables will be loaded into your data model. You can then create relationships between them in the **Model View**, for example, by linking a `CustomerID` from your SQL Server `Sales` table to the `CustomerID` in the Google Sheets `Customer Info` table.

---

### 19. What is the difference between galaxy and star schema?

Both are data modeling designs used in data warehousing and business intelligence. The Star Schema is the foundation.

**Star Schema**

- **Structure:** Consists of one central **Fact Table** surrounded by several **Dimension Tables**.
- **Fact Table:** Contains quantitative business data (the "facts" or measures), like `SalesAmount`, `Quantity`, `Cost`. It also contains foreign keys to the dimension tables.
- **Dimension Tables:** Contain descriptive attributes (the "dimensions"), like `Product Name`, `Customer City`, `Date`. They have a primary key that is referenced by the fact table.
- **Analogy:** It looks like a star, with the fact table at the center and the dimension tables as the points of the star.
- **Characteristics:** Simple, easy to understand, and highly optimized for the query performance of BI tools like Power BI. All relationships are one-to-many from the dimension to the fact table.

**Galaxy Schema (also called Fact Constellation Schema)**

- **Structure:** A more complex design that consists of **multiple Fact Tables** that share one or more **common Dimension Tables**.
- **Analogy:** It's like a collection of stars (a "galaxy"), where the shared dimensions act as the link between different fact tables (the star systems).
- **Example:** You might have a `Sales` fact table and a `Shipping` fact table. Both of these fact tables could be linked to the same `DimProduct`, `DimDate`, and `DimStore` dimension tables. This allows you to analyze sales and shipping metrics together by product, date, or store.
- **Use Case:** Used when you want to model and analyze multiple, related business processes in a single data model.

**In Summary:** A Star Schema has one fact table. A Galaxy Schema has two or more fact tables that share dimensions.

---

### 20. What is a composite model?

A **Composite Model** is a feature in Power BI that allows a single report to seamlessly combine data from different **Data Connectivity modes**. Specifically, it lets you combine:

- **DirectQuery** sources (e.g., a live connection to a massive SQL Data Warehouse).
- **Import** sources (e.g., an imported Excel file with budget data).
- **Power BI datasets** (Live Connection).

**Why is it important?**

Before composite models, a Power BI report was limited to either entirely Import mode or a single DirectQuery source. You couldn't mix them.

**Key Capabilities of a Composite Model:**

1. **Mix and Match:** You can have a DirectQuery connection to your main enterprise data warehouse for real-time fact data, and also _import_ a smaller Excel/CSV file (like sales targets or product groupings) and create relationships between the two.
2. **Best of Both Worlds:** This gives you the performance and flexibility of Import mode for smaller dimension tables, combined with the scalability and real-time nature of DirectQuery for huge fact tables.
3. **Many-to-Many Relationships:** Composite models were the feature that enabled the robust implementation of many-to-many relationships in Power BI.
4. **Enhanced Performance:** You can change the storage mode of individual tables. For example, a dimension table from a DirectQuery source can be set to "Dual" mode, meaning it can act as either Import or DirectQuery, which Power BI intelligently chooses to optimize query performance.

A composite model is an advanced data modeling technique that provides ultimate flexibility for complex enterprise-level reporting scenarios.

---

### 21. What filter have you used?

This is an open-ended question, so the best way to answer is to demonstrate knowledge of the _types_ of filters available in Power BI and when you would use each.

"In my projects, I use a combination of filters depending on the goal. My filtering strategy typically includes:"

1. **User-Facing Interactive Filters (Slicers):**
    - "For the primary dimensions that users need to interact with, like **Date**, **Region**, or **Product Category**, I always use **Slicers** directly on the report canvas. This provides an intuitive and obvious way for users to explore the data."
2. **Backend/Developer Filters (Filters Pane):**
    - **Page-Level Filters:** "If an entire report page is dedicated to a specific segment, like 'Executive Overview' or a single country, I apply a **Page-Level Filter** in the Filters Pane. This ensures all visuals on that page are correctly filtered without cluttering the canvas."
    - **Report-Level Filters:** "I use **Report-Level Filters** for conditions that must apply to the entire report, such as filtering out test data, excluding records before a certain go-live date, or focusing the entire report on the current fiscal year."
    - **Visual-Level Filters:** "For specific charts, I use **Visual-Level Filters**. A common example is creating a 'Top 10 Products' bar chart by applying a 'Top N' filter to just that one visual."
3. **DAX-based Filters (in Measures):**
    - "Inside my DAX measures, I extensively use the `**CALCULATE**` function to apply complex filter logic that can't be achieved with simple slicers. For example, to create a 'Year-over-Year Sales Growth' measure, I use `CALCULATE` combined with time intelligence functions like `SAMEPERIODLASTYEAR` to modify the date filter context."
4. **Security Filters (Row-Level Security):**
    - "For reports containing sensitive data, I implement **Row-Level Security (RLS)**. I create roles (e.g., 'Regional Manager - North') and use DAX rules to filter the data so that when a user from that role logs in, they automatically see only the data for their specific region. This is the most secure way to manage data access."

By explaining the different types and their use cases, you demonstrate a comprehensive understanding of how to control the data presented to the user.

---

### 22. What is the difference between import and DirectQuery connectivity mode? Which one is better?

This is a fundamental choice when connecting to a data source. There is no single "better" one; the best choice depends entirely on the project's requirements for performance, data size, and data freshness.

|   |   |   |
|---|---|---|
|Feature|**Import Mode**|**DirectQuery Mode**|
|**How it Works**|A **copy** of the data is loaded, compressed, and stored in-memory in the Power BI file (PBIX) using the VertiPaq engine.|**No data is copied.** Power BI stores only the metadata (table and column names). Every visual interaction sends a live query to the source database.|
|**Performance**|**Very Fast.** Queries are resolved by the highly optimized in-memory VertiPaq engine.|**Variable/Slower.** Performance is entirely dependent on the speed and optimization of the underlying source database. Poorly written DAX can translate to slow SQL.|
|**Data Freshness**|Data is **static** and only as fresh as the last refresh. Requires a manual or scheduled refresh to be updated.|Data is **near real-time.** Every visual shows the latest data from the source.|
|**Data Size**|**Limited.** The dataset size is constrained by memory and license type (e.g., 1 GB for Pro, up to 400 GB for Premium).|**Virtually unlimited.** Can handle datasets that are terabytes in size, as the data remains in the source system.|
|**DAX & Power Query**|**Full functionality.** All M (Power Query) and DAX functions are supported.|**Limited functionality.** Some transformations and DAX functions are not supported because they cannot be translated into a native query for the source database.|
|**Source Load**|Puts a heavy load on the source **only during refresh**.|Puts a **constant, interactive load** on the source database. A popular report can generate thousands of queries.|

**Which one is better? The Verdict:**

- **Use Import Mode (Default & Preferred):**
    - When performance is the top priority.
    - When the dataset size is manageable (from a few MB up to a few GB).
    - When near real-time data is not a strict requirement (daily or hourly refreshes are acceptable).
    - When you need the full power of Power Query and DAX.
    - **This covers 80-90% of all Power BI use cases.**
- **Use DirectQuery Mode:**
    - When the source dataset is **too large to import** into memory (e.g., many billions of rows).
    - When there is a strict business requirement for **real-time or near real-time data**.
    - When data residency policies prevent data from being copied out of the source system.
- **Use a Composite Model (The Hybrid Solution):**
    - For the best of both worlds, use a composite model. Keep massive fact tables in DirectQuery and import smaller, related dimension tables to get the best possible performance and flexibility.
